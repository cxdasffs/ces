import os
import time
import json
import asyncio
import httpx
from loguru import logger

# è®¾å®šæ—¥å¿—æ–‡ä»¶å’Œå­˜å‚¨è·¯å¾„
script_dir = os.getcwd()
valid_links_path = os.path.join(script_dir, "valid_links.txt")
config_file_path = os.path.join(script_dir, "config.json")

# è®¾ç½®æ—¥å¿—è®°å½•
logger.add("script_log.txt", format="{time} - {message}", level="INFO")

# ç¡®ä¿æ–‡ä»¶å­˜åœ¨
if not os.path.exists(valid_links_path):
    open(valid_links_path, "w").close()

# **åŠ è½½é…ç½®æ–‡ä»¶**
def load_config():
    if os.path.exists(config_file_path):
        try:
            with open(config_file_path, "r") as config_file:
                config = json.load(config_file)
                if "last_processed_id" not in config:
                    config["last_processed_id"] = config.get("start_id", 5444619000) - 1
                    save_config(**config)
                return config
        except json.JSONDecodeError:
            logger.error("é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼Œå°†é‡æ–°åˆ›å»ºé…ç½®æ–‡ä»¶...")
            return create_default_config()
    else:
        logger.info("é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ­£åœ¨åˆ›å»ºé…ç½®æ–‡ä»¶å¹¶åˆå§‹åŒ–é»˜è®¤å€¼...")
        return create_default_config()

# **åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶**
def create_default_config():
    default_config = {
        "start_id": 5444619000,
        "end_id": 8444649000,
        "step": 1,
        "batch_size": 10000,
        "max_workers": 567,  # å›ºå®šå€¼
        "last_processed_id": 5444618999
    }
    save_config(**default_config)
    return default_config

# **ä¿å­˜é…ç½®æ–‡ä»¶**
def save_config(**config):
    with open(config_file_path, "w") as config_file:
        json.dump(config, config_file)
    logger.info(f"é…ç½®æ–‡ä»¶å·²ä¿å­˜ï¼š{config_file_path}")

# **å¼‚æ­¥æ£€æŸ¥å•ä¸ªç›¸å†Œ**
async def check_album_async(album_id, client):
    url = f"https://www.pailixiang.com/m/album_ia{album_id}.html"
    try:
        response = await client.head(url, timeout=2)
        if response.status_code == 200:
            response = await client.get(url, timeout=5)
            if "ç›¸å†Œä¸å­˜åœ¨æˆ–å·²åˆ é™¤" not in response.text:
                return url
    except (httpx.RequestError, httpx.TimeoutError):
        pass
    return None

# **å¼‚æ­¥å¤„ç†æ‰¹æ¬¡**
async def process_batch_async(batch_start_id, batch_end_id):
    global success_count, failure_count, processed_count, current_start_id, last_processed_id
    batch_valid_links = []
    last_print_time = time.time()  # è®°å½•ä¸Šæ¬¡æ‰“å°æ—¶é—´

    async with httpx.AsyncClient(http2=True) as client:
        album_ids = list(range(batch_start_id, batch_end_id, step))
        results = await asyncio.gather(*[check_album_async(id, client) for id in album_ids])
        for result in results:
            processed_count += 1
            if result:
                batch_valid_links.append(result)
                success_count += 1
            else:
                failure_count += 1

            # æ¯å¤„ç† 100 ä¸ªä»»åŠ¡æˆ–ä»»åŠ¡å®Œæˆæ—¶ï¼Œè¾“å‡ºè¿›åº¦
            if processed_count % 100 == 0 or processed_count == total_tasks:
                current_time = time.time()
                elapsed_time = current_time - start_time  # æ€»è¿è¡Œæ—¶é—´
                time_since_last_print = current_time - last_print_time  # è·ç¦»ä¸Šæ¬¡æ‰“å°çš„æ—¶é—´
                last_print_time = current_time

                # è®¡ç®—é€Ÿåº¦ï¼ˆæ¯ç§’å¤„ç†çš„ä»»åŠ¡æ•°é‡ï¼‰
                if time_since_last_print > 0:
                    speed = 100 / time_since_last_print  # æ¯ 100 ä¸ªä»»åŠ¡çš„é€Ÿåº¦
                else:
                    speed = 0

                print(
                    f"\nğŸš€ å·²è¿è¡Œ {elapsed_time:.2f} ç§’ | "
                    f"å·²å¤„ç† {processed_count}/{total_tasks} ä¸ªç›¸å†Œ ID | "
                    f"é€Ÿåº¦: {speed:.2f} ä¸ª/ç§’ | "
                    f"âœ… æˆåŠŸ: {success_count} | âŒ å¤±è´¥: {failure_count}",
                    flush=True
                )

    if batch_valid_links:
        with open(valid_links_path, "a") as file:
            file.write("\n".join(batch_valid_links) + "\n")
        logger.info(f"æ‰¾åˆ° {len(batch_valid_links)} ä¸ªæœ‰æ•ˆç›¸å†Œ")

    current_start_id = last_processed_id + 1

# **ä¸»å‡½æ•°**
async def main():
    global current_start_id
    logger.info(f"ğŸ“Œ ä» ID {current_start_id} å¼€å§‹å¤„ç†")
    while current_start_id <= end_id:
        current_end_id = min(current_start_id + batch_size, end_id)
        logger.info(f"ğŸ“Œ å¤„ç† ID èŒƒå›´: {current_start_id} - {current_end_id}")
        await process_batch_async(current_start_id, current_end_id)
        current_start_id = current_end_id

# **è¿è¡Œå¼‚æ­¥ä¸»å‡½æ•°**
if __name__ == "__main__":
    config = load_config()
    start_id = config["start_id"]
    end_id = config["end_id"]
    step = config["step"]
    batch_size = config["batch_size"]
    MAX_WORKERS = config["max_workers"]  # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å›ºå®šå€¼
    last_processed_id = config["last_processed_id"]
    current_start_id = last_processed_id + 1

    # è®¡ç®—æ€»ä»»åŠ¡æ•°
    total_tasks = len(range(current_start_id, end_id, step))
    logger.info(f"ğŸ“Œ é¢„è®¡å¤„ç† {total_tasks} ä¸ªç›¸å†Œ ID")

    success_count = 0
    failure_count = 0
    processed_count = 0
    start_time = time.time()

    asyncio.run(main())

    # æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯
    total_time = time.time() - start_time
    average_speed = processed_count / total_time if total_time > 0 else 0
    logger.info(f"æ€»è¿è¡Œæ—¶é—´: {total_time:.2f} ç§’")
    logger.info(f"æ€»å…±æ‰¾åˆ° {success_count} ä¸ªæœ‰æ•ˆç›¸å†Œ")
    logger.info(f"å¤±è´¥ {failure_count} ä¸ª")
    logger.info(f"å¹³å‡æ¯ç§’å¤„ç† {average_speed:.2f} ä¸ªç›¸å†Œ ID")